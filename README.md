# LIKES. High-load applications.


Мы рассматриваем архитектуру счётчиков на высоконагруженных интернет приложениях. 

Счётчики - это лайки, просмотры, посещения, репосты, количество новых сообщений, оповещений и т.д.

## Задача

Необходимо считывать количество лайков с данной страницы (поста) и предоставлять информацию пользователю в «реальном времени», т.е. время отклика сервера должно быть минимально.

## Архитектура

Различают монолитную архитектуру и архитектуру микросервисов. В первом случае выше надежность, согласованность кода, данных, во втором случае выше гибкость, доступность. Многие высоконагруженные приложения – социальные сети, используют микросервисную архитектуру. 

Мы выделили систему обработки счётчиков в отдельный микросервис, который снимает часть нагрузки с других сервисов, занимаясь хранением внутренних данных.

Данные счётчиков во многих социальных сетях актуальны постоянно. Поэтому прямое обращение к базе данных за каждым небольшим объёмом информации может стать узким горлом в работе системы. Memcache - эффиктивная технология для работы с key-value базами данных. Эта система позволит обращаться к актуальным данным значительно быстрее.

## Проблемы

##### Проблема падения сервера memcache

Будем рассматривать кластер memcache, каждый сервер обрабатывает группу с номером hash(post_id, like_owner_id) % server_amount. Проблема заключается в том, что в memcache могут содеражаться несохранённые в основной базе данные, когда сервер падает.

Имеет смысл логирование операций до попадания их в базу данных. В случае отключения можно равномерно распределить пространство хэшей потерянного сервера между оставшимися машинами кластера и восстановить данные из лога.

При этом у нас усложняется алгоритм распределения хэшей по серверам. Это не большая проблема, если организовано запланированное перехэширование данных в момент спада нагрузки.

Восстановление из лога может являться самым долгим процессом. Но его можно выполнять параллельно с основной работой. Если пользователь обратится к потерянным хэшам на запись, то есть возможность записать данные, а по окончании восстановления объединить старые и новые данные. При обращении на чтение можно подгрузить данные из основной базы. В данном случае мы нарушаем consistency из CAP теоремы. Но это временные меры, которые могут не быть проблемными при решении некоторых задач.

##### Проблема 10 друзей

Рассматривается принение схемы в социальной сети. Требуется, чтобы читатели поста, приняв уведомление о поставленном лайке, знали, является ли его владелец другом или нет? Аналогичная ситуация при запросе поста с сервера. В таком случае проверяются все владельцы лайков для одного поста.

Предлагается следующее решение. Когда в микросервисе постов формируется уведомление (или пост), инициализируется некоторая структура структура для хранения отношений между пользователями. Такая структура может быть двудольным графом отношений, либо булева матрица, неполная матрицы смежности: на строках которой стоят читатели поста, по колонкам - владельцы лайков. Отправляется запрос к микросервису пользователей, в котором проверяется для каждой ячейки матрицы, является ли пара пользователей друзьями. Микросервис постов, приняв эту информацию, в запросе к клиентам добавляет булево поле: друг или нет.

Возникает вопрос о времени выполнения этой проверки. Возможно имеет смысл разослать уведомления всем читателям поста, не дожидаясь микросервис пользователей, а информацию о друзьях отправить с некоторой задержкой.

##### Master - slave

Положим, что случились проблемы с master сервером (или кластером), после которых он не доступен для остальных участников процесса. При этом остались неотправленные данные в slave хранилища. 

Аналогично решению проблемы падения сервера memcache, можно применить логирование операций. Это позволит восстановить потерянную информацию на другом кластере и установить его как master. 

В качестве оптимизации можно использовать спецальный slave, который будет забирать себе информацию, как нормальный slave, но в случае падения мастера, может стать сам мастером, подгрузив недоотправленную инфу из лога. Это позволит не восстанавливать всю инфомацию с других хранилищ, а только из лога.

Можно сделать пул кластеров, которые будут работать по протоколу RAFT.



---

* [Алёна Смирнова](https://github.com/alsmirnova)
* [Георгий Кубрин](https://github.com/Georgiy0)
* [Владислав Анисимов](https://github.com/ShittyWizard)
* [Никита Супротивный](https://github.com/NSuprotivniy)

